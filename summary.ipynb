{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "summary-reset.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scienclick/ScikitLearnPipeline4NLP/blob/master/summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Wt831xiVAt"
      },
      "source": [
        "## for data\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns## for processing\n",
        "import re\n",
        "import nltk## for bag-of-words\n",
        "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing## for explainer\n",
        "# from lime import lime_text## for word embedding\n",
        "import gensim\n",
        "import gensim.downloader as gensim_api## for deep learning\n",
        "# from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
        "# from tensorflow.keras import backend as K## for bert language model\n",
        "import sklearn.feature_selection\n",
        "# import transformers\n",
        "from zeugma import EmbeddingTransformer\n",
        "from symspellpy import SymSpell,Verbosity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set(rc={'figure.figsize':(20,12)})\n",
        "from sklearn.metrics import plot_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXLdgUTGiVAu"
      },
      "source": [
        "def tokenize(text):\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())  # normalize case and remove punctuation\n",
        "    tokens = word_tokenize(text)  # tokenize text\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # lemmatize andremove stop words\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DkJBnZbiVAv"
      },
      "source": [
        "## populating dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1BmLL0viVAv"
      },
      "source": [
        "#populating domain dictionary\n",
        "from symspellpy import SymSpell,Verbosity\n",
        "path = 'new 8.txt'\n",
        "symspell = SymSpell()\n",
        "symspell.create_dictionary(path,encoding='utf-8')\n",
        "suggestions = symspell.lookup(\"casing\", Verbosity.CLOSEST,max_edit_distance=1, include_unknown=True)\n",
        "for suggestion in suggestions:\n",
        "    print(suggestion)\n",
        "\n",
        "# str(suggestions[0]).split(\",\")[0]\n",
        "# suggestions[0].term\n",
        "# suggestions[0].distance\n",
        "# suggestions[0].count\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwO144D6iVAw"
      },
      "source": [
        "## Reading data and investigating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8nA5bhTiVAw"
      },
      "source": [
        "# <editor-fold desc=\"Read data full \">\n",
        "data = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ks3l8j7iVAw"
      },
      "source": [
        "data.head()\n",
        "data.columns\n",
        "data.isnull().sum()\n",
        "data[\"Activity\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIqkBfNmiVAw"
      },
      "source": [
        "data.groupby('Activity')[\"Description\"].count().plot.barh(ylim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xC4WqdWiVAx"
      },
      "source": [
        "# percentage\n",
        "target = data[\"Activity\"]\n",
        "counter = Counter(target)\n",
        "for k, v in counter.items():\n",
        "    per = v / len(target) * 100\n",
        "    print('Class=%s\\t, Count=  %d, \\tPercentage=  %.3f%%                 ' % (k, v, per))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-ZaDuV_iVAx"
      },
      "source": [
        "data[\"Activity\"].value_counts()\n",
        "data[\"digits\"] = data[\"Description\"].apply(lambda y: sum(c.isdigit() for c in y))\n",
        "data[\"letters\"] = data[\"Description\"].apply(lambda y: sum(c.isalpha() for c in y))\n",
        "data[\"others\"] = data[\"Description\"].apply(\n",
        "    lambda y: len(y) - sum(c.isalpha() for c in y) - sum(c.isdigit() for c in y) - sum(c.isspace() for c in y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6zV5PLCiVAx"
      },
      "source": [
        "ax = sns.boxplot(x=\"Activity\", y=\"others\",data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7vYjpwPiVAy"
      },
      "source": [
        "ax = sns.boxplot(x=\"Activity\", y=\"digits\", data=data)\n",
        "# ax = sns.boxplot(x=\"Activity\", y=\"letters\", data=data)\n",
        "# ax = sns.boxplot(x=\"Activity\", y=\"Hours\", data=data)\n",
        "# data[\"Hours\"].plot(kind=\"hist\", rotation=\"vertical\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC5UqwOAiVAy"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['Description'], data['label'], random_state=444)\n",
        "print('rows in the original data set: {}'.format(data.shape[0]))\n",
        "print('rows in the training set: {}'.format(X_train.shape[0]))\n",
        "print('rows in the test set: {}'.format(X_test.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9hqVxQhiVAy"
      },
      "source": [
        "## Reading Odd words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO7vZ_0SiVAy"
      },
      "source": [
        "master_str=' '.join(data[\"Description\"])\n",
        "master_str=re.sub(r\"[^a-zA-Z]\", \" \", master_str.lower())\n",
        "master_tokenized=tokenize(master_str)\n",
        "master_tokenized=[t for t in master_tokenized if t not in stop_words]\n",
        "master= Counter(master_tokenized)\n",
        "# len(master)\n",
        "# master= Counter(master_tokenized).most_common()\n",
        "# len(master)\n",
        "\n",
        "odd_list=[(k,val) for k,val in master.items() if k not in symspell.words.keys()]\n",
        "\n",
        "len(odd_list)\n",
        "odd_list2=[(r[0],r[1]) for r in odd_list if symspell.lookup(r[0], Verbosity.CLOSEST,max_edit_distance=1, include_unknown=False)==[]]\n",
        "len(odd_list2)\n",
        "\n",
        "odd_list_sorted=sorted(odd_list2, key=lambda tup: tup[1])\n",
        "odd_list_rare=[(r[0],r[1]) for r in odd_list2 if r[1]<2]\n",
        "len(odd_list_rare)\n",
        "final_odds=[r[0] for r in odd_list_rare]\n",
        "\n",
        "odd_str=\" \".join(final_odds)\n",
        "\n",
        "\n",
        "len(master)\n",
        "len(final_odds)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJ3k-UliVAz"
      },
      "source": [
        "### simple models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbuSmW0BiVAz"
      },
      "source": [
        "#this function is suitable when we use all the data\n",
        "def evaluate_model(X, y, model):\n",
        "    # define evaluation procedure\n",
        "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
        "    # evaluate model\n",
        "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRblK0QRiVAz"
      },
      "source": [
        "text_clf_lsvc2 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "#this is good to use all the data\n",
        "scores=evaluate_model(X_train,y_train,text_clf_lsvc2)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "#this is good when we put aside 20% for test\n",
        "text_clf_lsvc2.fit(X_train, y_train)\n",
        "predictions = text_clf_lsvc2.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ83LEE2iVA0"
      },
      "source": [
        "category_id_df = data[['Activity', \"label\"]].drop_duplicates().sort_values('label').set_index(\"label\")\n",
        "plot_confusion_matrix(text_clf_lsvc2, X_test, y_test, display_labels=list(category_id_df.Activity.values), cmap=\"hot\",\n",
        "                      xticks_rotation=\"vertical\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNojYP0XiVA0"
      },
      "source": [
        "plot_confusion_matrix(text_clf_lsvc2, X_test, y_test, normalize=\"true\",\n",
        "                      display_labels=list(category_id_df.Activity.values), cmap=\"hot\", xticks_rotation=\"vertical\",\n",
        "                      values_format=\".1f\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2cMddDuiVA0"
      },
      "source": [
        "plot_confusion_matrix(text_clf_lsvc2, X_test, y_test, normalize=\"pred\",\n",
        "                      display_labels=list(category_id_df.Activity.values), cmap=\"hot\", xticks_rotation=\"vertical\",\n",
        "                      values_format=\".1f\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD3Jk74wiVA0"
      },
      "source": [
        "### possible tunings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jApIeSpviVA1"
      },
      "source": [
        "('clf', LinearSVC(class_weight=\"balanced\"))\n",
        "\n",
        "('vect', CountVectorizer(ngram_range=(1,3)))\n",
        "\n",
        "('tfidf', TfidfTransformer(use_idf=True)),\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QfZ1HqyiVA1"
      },
      "source": [
        "## Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgpd3W5ZiVA1"
      },
      "source": [
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'vect__ngram_range': [(1, 1), (1, 2),(1,3),(1,4)],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'clf__alpha': (1e-1,1e-2, 1e-3,1e-4),\n",
        "}\n",
        "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
        "scores=evaluate_model(X_train,y_train,gs_clf)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "gs_clf = gs_clf.fit(X_train, y_train)\n",
        "predictions = gs_clf.best_estimator_.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzYAsQsgiVA1"
      },
      "source": [
        "## Feature union"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuSxORgGiVA1"
      },
      "source": [
        "class Digit_Extractor(BaseEstimator, TransformerMixin):\n",
        "    def text_length(self, text):\n",
        "        return len(text) - text.count(\" \")\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(lambda y: sum(c.isdigit() for c in y))\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "\n",
        "class Length_Extractor(BaseEstimator, TransformerMixin):\n",
        "    def text_length(self, text):\n",
        "        return len(text) - text.count(\" \")\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.text_length)\n",
        "\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "\n",
        "class Hour_Extractor(BaseEstimator, TransformerMixin):\n",
        "    def text_length(self, text):\n",
        "        return len(text) - text.count(\" \")\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X_tagged = data.iloc[X.index][\"Hours\"]\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "\n",
        "def model_pipeline2():\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "\n",
        "            ('text_pipeline', Pipeline([\n",
        "                ('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "\n",
        "            # ('digit_length', Digit_Extractor()),\n",
        "            # ('entire_length', Length_Extractor()),\n",
        "            ('hours', Hour_Extractor())\n",
        "\n",
        "        ])),\n",
        "\n",
        "        ('clf', LinearSVC())\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "model = model_pipeline2()\n",
        "model.fit(X_train, y_train);\n",
        "y_pred = model.predict(X_test)\n",
        "predictions = model.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERC43IjoiVA2"
      },
      "source": [
        "class num_Extractor(BaseEstimator, TransformerMixin):\n",
        "    def text_length(self, text):\n",
        "        text = re.sub(r\"[^0-9]\", \" \", text.lower())  # normalize case and remove punctuation\n",
        "        tokens = word_tokenize(text)  # tokenize text\n",
        "        return len(tokens)/len(text)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.text_length)\n",
        "\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "class slash_Extractor(BaseEstimator, TransformerMixin):\n",
        "    def text_length(self, text):\n",
        "        text = re.sub(r\"[^/]\", \" \", text.lower())  # normalize case and remove punctuation\n",
        "        tokens = word_tokenize(text)  # tokenize text\n",
        "        return len(tokens)/len(text)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.text_length)\n",
        "\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "class hr_Extractor(BaseEstimator, TransformerMixin):\n",
        "    def text_length(self, text):\n",
        "        text = re.sub(r\"[^:]\", \" \", text.lower())  # normalize case and remove punctuation\n",
        "        tokens = word_tokenize(text)  # tokenize text\n",
        "        return len(tokens)/len(text)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.text_length)\n",
        "\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "class inch_Extractor(BaseEstimator, TransformerMixin):\n",
        "    def text_length(self, text):\n",
        "        text = re.sub(r\"[^'\\\"]\", \" \", text.lower())  # normalize case and remove punctuation\n",
        "        tokens = word_tokenize(text)  # tokenize text\n",
        "        return 100*len(tokens)/len(text)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.text_length)\n",
        "\n",
        "        return pd.DataFrame(X_tagged)\n",
        "\n",
        "class sent_Extractor(BaseEstimator, TransformerMixin):\n",
        "    def text_length(self, text):\n",
        "        text = re.sub(r\"[^.?!]\", \" \", text.lower())  # normalize case and remove punctuation\n",
        "        tokens = word_tokenize(text)  # tokenize text\n",
        "        return len(tokens)\n",
        "\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tagged = pd.Series(X).apply(self.text_length)\n",
        "\n",
        "        return pd.DataFrame(X_tagged)\n",
        "    \n",
        "    \n",
        "def model_pipeline2():\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "            # ('inch', inch_Extractor()),\n",
        "            ('num', num_Extractor()),\n",
        "            ('slash', slash_Extractor()),\n",
        "            # ('hr', hr_Extractor()),\n",
        "            # ('sent', sent_Extractor()),\n",
        "\n",
        "            ('text_pipeline', Pipeline([\n",
        "                ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=2)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "            ])),\n",
        "\n",
        "            # ('digit_length', Digit_Extractor()),\n",
        "            # ('entire_length', Length_Extractor()),\n",
        "            # ('hours', Hour_Extractor())\n",
        "\n",
        "\n",
        "\n",
        "        ])),\n",
        "\n",
        "        ('clf', LinearSVC())\n",
        "    ])\n",
        "\n",
        "    return pipeline\n",
        "\n",
        "\n",
        "\n",
        "scores = evaluate_model(X_train, y_train, model_pipeline2())\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-X88IiriVA2"
      },
      "source": [
        "## comparison of classifiers along with grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq7pzi8EiVA3"
      },
      "source": [
        "\n",
        "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression, SGDClassifier, RidgeClassifierCV\n",
        "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier, \\\n",
        "    VotingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
        "from sklearn.semi_supervised import LabelSpreading\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sIkXqPGiVA3"
      },
      "source": [
        "classifiers = [\n",
        "    LinearSVC(),\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    SVC(gamma=2, C=1),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis(),\n",
        "    MultinomialNB(alpha=.001)\n",
        "]\n",
        "for clf in classifiers:\n",
        "    try:\n",
        "        print(\"------------------------------\",text_clf_lsvc2)\n",
        "\n",
        "        text_clf_lsvc2 = Pipeline([\n",
        "            ('vect', CountVectorizer),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', clf)\n",
        "        ])\n",
        "        parameters = {\n",
        "            'vect__ngram_range': [(1, 1), (1, 2),(1,3),(2,2),(2,3)],\n",
        "            'vect__stop_words': (\"english\",None),\n",
        "            'vect__tokenizer': (tokenize,None),\n",
        "            'tfidf__smooth_idf': (True, False),\n",
        "            'tfidf__use_idf': (True, False),\n",
        "            'tfidf__norm': ('l1','l2'),\n",
        "            # 'clf__alpha': (1e-1,1e-2, 1e-3,1e-4),\n",
        "        }\n",
        "        gs_clf = GridSearchCV(text_clf_lsvc2, parameters, cv=5, n_jobs=-1)\n",
        "        gs_clf.fit(X_train, y_train)\n",
        "        predictions = gs_clf.predict(X_test)\n",
        "        print(classification_report(y_test, predictions))\n",
        "        print(\"---------------------------------------------------------------------------\")\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7huc9023iVA3"
      },
      "source": [
        "models_ = [DummyClassifier(), ExtraTreeClassifier(), MultinomialNB(), SVC(),\n",
        "           BaggingClassifier(), BernoulliNB(), GaussianNB(), PassiveAggressiveClassifier(),\n",
        "           LogisticRegression(solver='lbfgs', multi_class='multinomial'), SGDClassifier(), AdaBoostClassifier(),\n",
        "           ExtraTreesClassifier(), RandomForestClassifier(), BaggingClassifier(), CalibratedClassifierCV(),\n",
        "           LGBMClassifier(), LinearSVC(), NuSVC(), XGBClassifier(), NearestCentroid(), KNeighborsClassifier(),\n",
        "           LinearDiscriminantAnalysis(), DecisionTreeClassifier(), LabelSpreading()]\n",
        "\n",
        "for mod in models_:\n",
        "    try:\n",
        "        text_clf_lsvc2 = Pipeline([\n",
        "            ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1, 3), min_df=2)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "            ('clf', mod)\n",
        "        ])\n",
        "        print(\"------------\", mod)\n",
        "        text_clf_lsvc2.fit(X_train, y_train)\n",
        "        scores=evaluate_model(X_train,y_train,text_clf_lsvc2)\n",
        "        print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "        # predictions = text_clf_lsvc2.predict(X_test)\n",
        "        # print(classification_report(y_test, predictions))\n",
        "    except Exception as e:\n",
        "        print(e.args)\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG5ufXUCiVA4"
      },
      "source": [
        "## stacking, ensembling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygSGO7_oiVA4"
      },
      "source": [
        "model1 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LinearSVC(C=1))\n",
        "])\n",
        "\n",
        "model2 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=1, max_df=0.95\n",
        "                             )),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB(fit_prior=True, alpha=0.001)),\n",
        "])\n",
        "# model3 = Pipeline([\n",
        "#     ('vect', CountVectorizer(tokenizer=tokenize_no_odd, ngram_range=(1, 3), min_df=1, max_df=0.95\n",
        "#                              )),\n",
        "#     ('tfidf', TfidfTransformer()),\n",
        "#     ('to_dense', DenseTransformer()),\n",
        "#     ('clf', GaussianNB()),\n",
        "# ])\n",
        "model3 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=1, max_df=0.95\n",
        "                             )),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', ExtraTreesClassifier()),\n",
        "])\n",
        "model4 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=1, max_df=0.95)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier()),\n",
        "])\n",
        "model5 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=1, max_df=0.95)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', CalibratedClassifierCV()),\n",
        "])\n",
        "\n",
        "estimators = []\n",
        "\n",
        "estimators.append(('m1', model1))\n",
        "estimators.append(('m2', model2))\n",
        "estimators.append(('m3', model3))\n",
        "estimators.append(('m4', model4))\n",
        "estimators.append(('m5', model5))\n",
        "\n",
        "level1 = LogisticRegression(solver=\"liblinear\")\n",
        "\n",
        "stacking = StackingClassifier(estimators=estimators, final_estimator=level1)\n",
        "scores = evaluate_model(X_train, y_train, stacking)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "stacking.fit(X_train, y_train)\n",
        "predictions = stacking.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRm7Q1e9iVA4"
      },
      "source": [
        "model1 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1, 3), min_df=2, max_df=.5, max_features=70000)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', LinearSVC(C=1))\n",
        "])\n",
        "\n",
        "model2 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1, 3), max_df=1)),\n",
        "    ('tfidf', TfidfTransformer(use_idf=False, smooth_idf=True)),\n",
        "    ('clf', MultinomialNB(fit_prior=True, alpha=0.001)),\n",
        "])\n",
        "model3 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1, 3), max_df=1)),\n",
        "    ('tfidf', TfidfTransformer(use_idf=False, smooth_idf=True)),\n",
        "    ('clf', GaussianNB()),\n",
        "])\n",
        "model4 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1, 3))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', SGDClassifier()),\n",
        "])\n",
        "model5 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize, ngram_range=(1, 3))),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', CalibratedClassifierCV()),\n",
        "])\n",
        "\n",
        "estimators = []\n",
        "\n",
        "estimators.append(('m1', model1))\n",
        "estimators.append(('m2', model2))\n",
        "# estimators.append(('m3', model3))\n",
        "estimators.append(('m4', model4))\n",
        "estimators.append(('m5', model5))\n",
        "\n",
        "level1 = LogisticRegression(solver=\"liblinear\")\n",
        "\n",
        "stacking = VotingClassifier(estimators=estimators, weights=(3, 1, 1, 2))\n",
        "scores = evaluate_model(X_train, y_train, stacking)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "stacking.fit(X_train, y_train)\n",
        "predictions = stacking.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "#Grid search\n",
        "stacking = VotingClassifier(estimators=estimators, weights=(3, 1, 1, 2))\n",
        "ensemble = VotingClassifier(estimators)\n",
        "params = {'voting': ['hard', ],\n",
        "          'weights': [(2, 1, 1, 1), (1, 1, 1, 2), (2, 1, 1, 2), (3, 1, 1, 1), (3, 1, 1, 3),\n",
        "                      (3, 1, 1, 2), (2, 1, 1, 3), (4, 1, 1, 2), (4, 1, 1, 3), (4, 1, 1, 4)]}\n",
        "\n",
        "gs_clf = GridSearchCV(ensemble, params, n_jobs=4, verbose=2, cv=3)\n",
        "gs_clf.fit(X_train, y_train)\n",
        "\n",
        "results = pd.DataFrame(gs_clf.cv_results_)[[\"mean_test_score\", \"params\", \"rank_test_score\"]]\n",
        "\n",
        "scores = evaluate_model(X_train, y_train, ensemble)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "predictions = ensemble.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ8X1QPXiVA5"
      },
      "source": [
        "## sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BJZ-I59iVA5"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as impipeline\n",
        "model5 = impipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_no_odd, ngram_range=(1, 3), min_df=1)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('over', SVMSMOTE()),\n",
        "    # ('under', RandomUnderSampler()),\n",
        "\n",
        "    ('clf', LinearSVC()),\n",
        "])\n",
        "scores=evaluate_model(X_train,y_train,model5)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "model5.fit(X_train, y_train)\n",
        "predictions = ensemble.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdeNWNLIiVA5"
      },
      "source": [
        "## Randomized search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8T2UrRRiVA5"
      },
      "source": [
        "params = {\n",
        "        'vect__max_features': [10000, 25000, 40000, 70000, 100000],\n",
        "        'vect__max_df': [0.5, .75, .85, 1],\n",
        "        'clf__criterion': ['gini', 'entropy'],\n",
        "          'clf__max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150],\n",
        "          \"clf__min_samples_split\": range(1, 10),\n",
        "          \"clf__min_samples_leaf\": range(1, 5)}\n",
        "\n",
        "text_clf_lsvc2 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_no_odd, ngram_range=(1, 3), min_df=1)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', DecisionTreeClassifier(ccp_alpha=.001)),\n",
        "])\n",
        "gs_clf = RandomizedSearchCV(text_clf_lsvc2, param_distributions=params, random_state=42, n_iter=5, cv=3, verbose=2,\n",
        "                            n_jobs=4, return_train_score=True)\n",
        "gs_clf.get_params().keys()\n",
        "scores = evaluate_model(X_train, y_train, gs_clf)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "gs_clf.fit(X_train, y_train)\n",
        "results = pd.DataFrame(gs_clf.cv_results_)[[\"params\", \"mean_test_score\", \"rank_test_score\"]]\n",
        "\n",
        "predictions = text_clf_lsvc2.predict(X_test)\n",
        "print(classification_report(y_test, predictions))\n",
        "gs_clf.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqflJr_9iVA5"
      },
      "source": [
        "## simple Optuna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXtI6SAgiVA6"
      },
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "def objective_func(trial):\n",
        "    n = trial.suggest_int(\"n_estimators\", 100, 500)\n",
        "\n",
        "    model5 = impipeline([\n",
        "        ('vect', CountVectorizer(tokenizer=tokenize_no_odd, ngram_range=(1, 3), min_df=1)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "\n",
        "        ('clf', ExtraTreesClassifier(n_estimators=n)),\n",
        "\n",
        "    ])\n",
        "\n",
        "    model5.fit(X_train, y_train)\n",
        "    return model5.score(X_test,y_test)\n",
        "\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                            sampler=TPESampler())\n",
        "study.optimize(objective_func, n_trials=5)\n",
        "best_trial = study.best_trial.value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def objective_func(trial):\n",
        "    c = trial.suggest_loguniform(\"C\", 1e-2, 1e+11)\n",
        "    mi = trial.suggest_loguniform(\"max_iter\", 1000, 3000)\n",
        "    p = trial.suggest_categorical(\"penalty\", \"l1\", \"l2\")\n",
        "    m = trial.suggest_categorical(\"multi_class\", \"ovr\", \"crammer_singer\")\n",
        "    i = trial.suggest_loguniform(\"intercept_scaling\", 1e-1, 2)\n",
        "    w = trial.suggest_categorical(\"class_weight\", \"balanced\", None)\n",
        "\n",
        "    model5 = impipeline([\n",
        "        ('vect', CountVectorizer(tokenizer=tokenize_no_odd, ngram_range=(1, 3), min_df=1)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "\n",
        "        ('clf', LinearSVC(C=c,max_iter=mi,penalty=p,multi_class=m,intercept_scaling=i,class_weight=w)),\n",
        "\n",
        "    ])\n",
        "\n",
        "    model5.fit(X_train, y_train)\n",
        "    return model5.score(X_test,y_test)\n",
        "\n",
        "study = optuna.create_study(direction='maximize',\n",
        "                            sampler=TPESampler())\n",
        "study.optimize(objective_func, n_trials=25)\n",
        "best_trial = study.best_trial.value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Bgp0c6iVA6"
      },
      "source": [
        "## nn on best results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS1mADL6iVA6"
      },
      "source": [
        "#first extracting the results\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "print(f\"*************************** simple SVC\")\n",
        "model_svc = impipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=2)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    # ('over', BorderlineSMOTE()),\n",
        "    # # ('under', RandomUnderSampler(sampling_strategy=w_under)),\n",
        "    #\n",
        "    # # ('select',SelectPercentile(chi2,percentile=10)),\n",
        "    # ('clf', LinearSVC()),\n",
        "    ('clf', LinearSVC()),\n",
        "#     ('clf', OneVsRestClassifier(LinearSVC())),\n",
        "    # ('clf', DecisionTreeClassifier(ccp_alpha=.001)),\n",
        "    # ('clf', xgb.sklearn.XGBClassifier(nthread=-1)),\n",
        "    # ('clf', xgb.XGBClassifier(treemethod=\"gpu_exact\",random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7,\n",
        "    #                           n_estimators=300)),\n",
        "])\n",
        "scores = evaluate_model(X_train, y_train, model_svc)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "\n",
        "y_pred_linear_svc = cross_val_predict(model_svc, X_train, y_train, cv=5,verbose=10,n_jobs=4)\n",
        "\n",
        "print(f\"*************************** simple MultinomialNB\")\n",
        "model_nb = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=1, max_df=0.95)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB( alpha=0.005)),\n",
        "])\n",
        "scores = evaluate_model(X_train, y_train, model_nb)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "y_pred_multinomialnb = cross_val_predict(model_nb, X_train, y_train, cv=5,verbose=10,n_jobs=4)\n",
        "print(f\"*************************** simple XG\")\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "model_xg = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=1, max_df=0.95)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', XGBClassifier(\n",
        "                          learning_rate =0.1,\n",
        "                          n_estimators=1000,\n",
        "                          max_depth=5,\n",
        "                          min_child_weight=1,\n",
        "                          gamma=0,\n",
        "                          subsample=0.8,\n",
        "                          colsample_bytree=0.8,\n",
        "                          objective= 'multi:softmax',\n",
        "                          nthread=2,\n",
        "                          scale_pos_weight=1,\n",
        "                          seed=27)),\n",
        "])\n",
        "scores = evaluate_model(X_train, y_train, model_xg)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "\n",
        "y_pred_xg = cross_val_predict(model_xg, X_train, y_train, cv=5,verbose=10,n_jobs=4)\n",
        "print(f\"*************************** simple SGD\")\n",
        "model_sgd = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=1, max_df=0.95)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "\n",
        "    ('clf', SGDClassifier(alpha=0.000035)),\n",
        "])\n",
        "scores = evaluate_model(X_train, y_train, model_sgd)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "y_pred_sgd = cross_val_predict(model_sgd, X_train, y_train, cv=5,verbose=10,n_jobs=4)\n",
        "\n",
        "\n",
        "print(f\"*************************** simple CalibratedClassifierCV\")\n",
        "model5 = Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_snow, ngram_range=(1, 3), min_df=1, max_df=0.95)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', CalibratedClassifierCV(base_estimator=xgb.XGBClassifier)),\n",
        "])\n",
        "scores = evaluate_model(X_train, y_train, model5)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "y_pred_calibrated = cross_val_predict(model5, X_train, y_train, cv=5,verbose=10,n_jobs=4)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"*************************** simple ExtraTreesClassifier\")\n",
        "model_xtree =Pipeline([\n",
        "    ('vect', CountVectorizer(tokenizer=tokenize_no_odd, ngram_range=(1, 3), min_df=1, max_df=0.95\n",
        "                             )),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', ExtraTreesClassifier(n_estimators= 100)),\n",
        "])\n",
        "scores = evaluate_model(X_train, y_train, model_xtree)\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
        "y_pred_extratree = cross_val_predict(model_xtree, X_train, y_train, cv=5,verbose=10,n_jobs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW9rQQ_IiVA6"
      },
      "source": [
        "#### create nn with most repeated words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AwD1LJdiVA6"
      },
      "source": [
        "res=pd.DataFrame(data={\"svc\":y_pred_linear_svc,\"nb\":y_pred_multinomialnb,\"xg\":y_pred_xg,\"sgd\":y_pred_sgd,\n",
        "                       \"xtree\":y_pred_extratree,\"y\":y_train,\n",
        "                       \"hours\":data.Hours,\"len_\":data[\"Description\"].apply(lambda y:len(y)),\n",
        "                       \"c\":data[\"Description\"].apply(lambda y:[i[0] for i in Counter(tokenize_snow_no_digits_0(y)).most_common()][:2])\n",
        "                       })\n",
        "res[\"numc\"]=data[\"Description\"].apply(lambda y: 1 if len(tokenize_inverse(y))>0  else 0)\n",
        "res[\"sent\"]=data[\"Description\"].apply(lambda y: 1 if  len(tokenize_sentense(y))>0 else 0)\n",
        "res[\"hr\"]=data[\"Description\"].apply(lambda y: 1 if len(tokenize_hours(y))>0 else 0)\n",
        "res[\"slash\"]=data[\"Description\"].apply(lambda y:1 if  len(tokenize_slash(y))>0 else 0)\n",
        "res[\"inch\"]=data[\"Description\"].apply(lambda y: 1 if len(tokenize_inch(y))>0 else 0)\n",
        "\n",
        "res.sample(50,random_state=7)\n",
        "\n",
        "res[\"predicted\"]=(res.y==res.svc) | (res.y==res.nb)| (res.y==res.xg) | (res.y==res.sgd) | (res.y==res.xtree)\n",
        "res[\"predicted\"].value_counts()\n",
        "wrongs=res[res[\"predicted\"]==False]\n",
        "wrongs.sample(50)\n",
        "wrongs[\"svc\"]==6\n",
        "wrongs[\"nb\"]==6\n",
        "wrongs[\"xg\"]==6\n",
        "wrongs[\"sgd\"]==6\n",
        "wrongs[\"xtree\"]==6\n",
        "wrong_6=wrongs[(wrongs[\"svc\"]==6)&(wrongs[\"xg\"]==6)&(wrongs[\"nb\"]==6)&(wrongs[\"sgd\"]==6)&(wrongs[\"xtree\"]==6)]\n",
        "wrong_6.sample(60)\n",
        "\n",
        "cat_cols = ['svc', 'nb', 'xg','sgd','xtree']\n",
        "cont_cols = ['hours','len_',\"numc\",\"sent\",\"hr\",\"slash\",\"inch\"]\n",
        "cats = np.stack([res[col].values for col in cat_cols], 1)\n",
        "import torch\n",
        "cats = torch.tensor(cats, dtype=torch.int64)\n",
        "res[\"cc\"]=res[\"c\"].apply(lambda y: \" \".join(y))\n",
        "def model_pipeline():\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        # ('tfidf', TfidfTransformer()),\n",
        "    ])\n",
        "    return pipeline\n",
        "model_t = model_pipeline()\n",
        "transformed=model_t.fit(res.cc);\n",
        "vals=transformed.transform(res.cc).toarray()\n",
        "# vals\n",
        "features=model_t.steps[0][1].get_feature_names()\n",
        "frequency_matrix_count_0 = pd.DataFrame(vals, columns=features)\n",
        "frequency_matrix_count_0\n",
        "# frequency_matrix_count_0[cont_cols[0]]=res[cont_cols[0]]\n",
        "# frequency_matrix_count_0[cont_cols[1]]=res[cont_cols[1]]\n",
        "# frequency_matrix_count_0[cont_cols[2]]=res[cont_cols[2]]\n",
        "# frequency_matrix_count_0[cont_cols[3]]=res[cont_cols[3]]\n",
        "# frequency_matrix_count_0[cont_cols[4]]=res[cont_cols[4]]\n",
        "# frequency_matrix_count_0[cont_cols[5]]=res[cont_cols[5]]\n",
        "# frequency_matrix_count_0[cont_cols[6]]=res[cont_cols[6]]\n",
        "conts=frequency_matrix_count_0\n",
        "conts = torch.tensor(conts.values, dtype=torch.float)\n",
        "cats.shape\n",
        "conts.shape\n",
        "emb_szs = [(24,32),(24,32),(24,32),(24,32),(24,32)]\n",
        "emb_szs\n",
        "import torch.nn as nn\n",
        "class TabularModel(nn.Module):\n",
        "    def __init__(self, emb_szs, n_cont, out_sz, layers, p=0.5):\n",
        "        super().__init__()\n",
        "        self.embeds = nn.ModuleList([nn.Embedding(ni, nf) for ni,nf in emb_szs])\n",
        "        self.emb_drop = nn.Dropout(p)\n",
        "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "        layerlist = []\n",
        "        n_emb = sum((nf for ni,nf in emb_szs))\n",
        "        n_in = n_emb + n_cont\n",
        "        for i in layers:\n",
        "            layerlist.append(nn.Linear(n_in,i))\n",
        "            layerlist.append(nn.ReLU(inplace=True))\n",
        "            layerlist.append(nn.BatchNorm1d(i))\n",
        "            layerlist.append(nn.Dropout(p))\n",
        "            n_in = i\n",
        "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
        "        self.layers = nn.Sequential(*layerlist)\n",
        "    def forward(self, x_cat, x_cont):\n",
        "        embeddings = []\n",
        "        for i,e in enumerate(self.embeds):\n",
        "            embeddings.append(e(x_cat[:,i]))\n",
        "        x = torch.cat(embeddings, 1)\n",
        "        x = self.emb_drop(x)\n",
        "        x_cont = self.bn_cont(x_cont)\n",
        "        x = torch.cat([x, x_cont], 1)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(33)\n",
        "model = TabularModel(emb_szs, conts.shape[1], 24, [400,200,100,50], p=0.5) # out_sz = 2\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model.to(device)\n",
        "batch_size = 20419\n",
        "test_size = 4000\n",
        "cat_train = cats[:batch_size-test_size]\n",
        "cat_test = cats[batch_size-test_size:batch_size]\n",
        "con_train = conts[:batch_size-test_size]\n",
        "con_test = conts[batch_size-test_size:batch_size]\n",
        "yy_train = data[\"label\"][:-test_size] if test_size!=0 else data[\"label\"]\n",
        "yy_test = data[\"label\"][-test_size:]\n",
        "yy_train = torch.tensor(yy_train.values, dtype=torch.int64,device=device)\n",
        "len(cat_train)\n",
        "len(yy_train)\n",
        "len(con_train)\n",
        "cat_train.shape\n",
        "import time\n",
        "start_time = time.time()\n",
        "epochs = 1800\n",
        "# epochs = 10000\n",
        "losses = []\n",
        "optimizer.zero_grad()\n",
        "for i in range(epochs):\n",
        "    i+=1\n",
        "    y_pred = model(cat_train.to(device), con_train.to(device))\n",
        "    y_pred.size\n",
        "    loss = criterion(y_pred, yy_train)\n",
        "    losses.append(loss)\n",
        "    # a neat trick to save screen space:\n",
        "    if i%100 == 1:\n",
        "        print(f'epoch: {i:3}  loss: {loss.item():10.8f}')\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(f'epoch: {i:3}  loss: {loss.item():10.8f}') # print the last line\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed\n",
        "with torch.no_grad():\n",
        "    preds=model.cpu()(cat_test,con_test)\n",
        "    preds.shape\n",
        "    t,i=torch.max(nn.functional.log_softmax(preds,dim=1),1)\n",
        "    # t,i=torch.max(preds,1)\n",
        "    print(classification_report(yy_test, i.cpu()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mra7PF2GiVA7"
      },
      "source": [
        "##  storing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5XyI-v8iVA7"
      },
      "source": [
        "data_test = pd.read_excel(\"/test.xlsx\")\n",
        "X_ = data_test[\"Description\"]\n",
        "pd.DataFrame({\"prediction\": stacking.predict(X_)}).to_csv(\"zz.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJIRasb9iVA7"
      },
      "source": [
        "data_test = pd.read_excel(\"test.xlsx\")\n",
        "X_ = data_test[\"Description\"]\n",
        "model_svc.fit(X_train,y_train)\n",
        "model_xg.fit(X_train,y_train)\n",
        "model_nb.fit(X_train,y_train)\n",
        "model_sgd.fit(X_train,y_train)\n",
        "model_xtree.fit(X_train,y_train)\n",
        "\n",
        "svc_=model_svc.predict(X_)\n",
        "xg_=model_xg.predict(X_)\n",
        "nb_=model_nb.predict(X_)\n",
        "sgd_=model_sgd.predict(X_)\n",
        "xtree_=model_xtree.predict(X_)\n",
        "\n",
        "res_test=pd.DataFrame(data={\"svc\":svc_,\"nb\":nb_,\"xg\":xg_,\"sgd\":sgd_,\"xtree\":xtree_,\n",
        "                       \"c\":data_test[\"Description\"].apply(lambda y:[i[0] for i in Counter(tokenize_snow_no_digits_0(y)).most_common()][:2])\n",
        "                       })\n",
        "\n",
        "res_test.to_csv(\"/level2_test.csv\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cat_cols = ['svc', 'nb', 'xg','sgd','xtree']\n",
        "cats_pred = np.stack([res_test[col].values for col in cat_cols], 1)\n",
        "import torch\n",
        "cats_pred = torch.tensor(cats_pred, dtype=torch.int64)\n",
        "\n",
        "res_test[\"cc\"]=res_test[\"c\"].apply(lambda y: \" \".join(y))\n",
        "vals_test=transformed.transform(res_test.cc).toarray()\n",
        "# vals\n",
        "features=model_t.steps[0][1].get_feature_names()\n",
        "frequency_matrix_count_0 = pd.DataFrame(vals_test, columns=features)\n",
        "frequency_matrix_count_0\n",
        "# frequency_matrix_count_0[cont_cols[0]]=res[cont_cols[0]]\n",
        "# frequency_matrix_count_0[cont_cols[1]]=res[cont_cols[1]]\n",
        "conts_pred=frequency_matrix_count_0\n",
        "conts_pred = torch.tensor(conts_pred.values, dtype=torch.float)\n",
        "cats_pred.shape\n",
        "conts_pred.shape\n",
        "\n",
        "\n",
        "y_pred=model.cpu()(cats_pred,conts_pred)\n",
        "with torch.no_grad():\n",
        "    t,i=torch.max(nn.functional.log_softmax(y_pred.cpu(),dim = 1),1)\n",
        "    pd.DataFrame({\"prediction\": i}).to_csv(\"/torch.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}