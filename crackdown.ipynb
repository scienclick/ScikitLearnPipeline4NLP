{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "crackdown.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scienclick/ScikitLearnPipeline4NLP/blob/master/crackdown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z89w66N9olJD"
      },
      "source": [
        "# <editor-fold desc=\"Import\">\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "from torchtext.legacy import data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from collections import Counter\n",
        "from symspellpy import SymSpell,Verbosity\n",
        "import torch.nn.functional as F\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "from util.dependencies import tokenize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from torchtext.legacy import data\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "import torch.optim as optim\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau,StepLR,MultiStepLR\n",
        "# </editor-fold>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMKZs_JyolJE"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjp10AEGolJF"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def tokenize(text):\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())  # normalize case and remove punctuation\n",
        "    tokens = word_tokenize(text)  # tokenize text\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # lemmatize andremove stop words\n",
        "    return tokens\n",
        "\n",
        "def classification_rep(X_test,y_test):\n",
        "    x_test=torch.Tensor(transformed.transform(X_test).toarray()).to(device)\n",
        "    with torch.no_grad():\n",
        "        preds=iris_model(x_test).cpu()\n",
        "        preds.shape\n",
        "        t,i=torch.max(nn.functional.log_softmax(preds,dim=1),1)\n",
        "        # t,i=torch.max(preds,1)\n",
        "        print(classification_report(y_test, i))\n",
        "        return i.numpy()\n",
        "    \n",
        "def plot_confusion_matrix_(data,predictions):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    import seaborn as sns\n",
        "    import matplotlib.pyplot as plt\n",
        "    category_id_df = data[['Activity', \"label\"]].drop_duplicates().sort_values('label').set_index(\"label\")\n",
        "    a_dict=category_id_df.to_dict()[\"Activity\"]\n",
        "    col=list(a_dict.keys())\n",
        "    col_=[a_dict[i] for i in col]\n",
        "    c=col_\n",
        "    y_test_=[a_dict[i] for i in y_test]\n",
        "    predictions_=[a_dict[i] for i in predictions]\n",
        "    conf_mat = confusion_matrix(y_test_, predictions_,labels=c)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plt_recall=100*conf_mat // conf_mat.sum(axis=1)\n",
        "    plt_prec=100*conf_mat // conf_mat.sum(axis=0)\n",
        "    plt.figure()\n",
        "    sns.heatmap(plt_recall, annot=True, fmt='d',xticklabels=c, yticklabels=c).set_title('Recall')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.figure()\n",
        "    sns.heatmap(plt_prec, annot=True, fmt='d',xticklabels=c, yticklabels=c).set_title('Precission')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.figure()\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='d',xticklabels=c, yticklabels=c).set_title('confusion matrix')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    \n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    # iterator=train_iterator\n",
        "    # batch.text.shape\n",
        "    # predictions.shape\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(batch.text)\n",
        "\n",
        "        # loss = nn.functional.nll_loss(nn.functional.log_softmax(predictions), batch.label)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "def train_(model, iterator, optimizer, criterion):\n",
        "    # iterator=train_iterator\n",
        "    # batch.text.shape\n",
        "    # predictions.shape\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(batch.text)\n",
        "\n",
        "        loss = nn.functional.nll_loss(nn.functional.log_softmax(predictions), batch.label)\n",
        "        # loss = criterion(predictions, batch.label)\n",
        "\n",
        "        acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "def evaluate(model, iterator, criterion):\n",
        "    # iterator=valid_iterator\n",
        "    # iterator=train_iterator\n",
        "    # batch.text\n",
        "\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            # print(batch.text)\n",
        "            predictions = model(batch.text)\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "\n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhiQxMWJolJG"
      },
      "source": [
        "## scikit-learn pytorch combination\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85tgMga1olJH"
      },
      "source": [
        "data = pd.read_excel(\"train.xlsx\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Description'], data['label'], random_state=444,test_size=.00001)\n",
        "print('rows in the original data set: {}'.format(data.shape[0]))\n",
        "print('rows in the training set: {}'.format(X_train.shape[0]))\n",
        "print('rows in the test set: {}'.format(X_test.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGPK4z0folJH"
      },
      "source": [
        "def model_pipeline():\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', CountVectorizer(tokenizer=tokenize,ngram_range=(1,3),min_df=4,max_df=.8)),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "    ])\n",
        "    return pipeline\n",
        "proc = model_pipeline()\n",
        "transformed=proc.fit(X_train);\n",
        "vals=transformed.transform(X_train).toarray()\n",
        "features=proc.steps[0][1].get_feature_names()\n",
        "frequency_matrix_count_1 = pd.DataFrame(vals, columns=features)\n",
        "frequency_matrix_count_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2XUu3ffolJH"
      },
      "source": [
        "iris=TensorDataset(torch.FloatTensor(frequency_matrix_count_1.values),torch.LongTensor(y_train))\n",
        "iris_loader=DataLoader(iris,batch_size=64,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASdv9MVHolJI"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, in_,h1,h2, out_):\n",
        "        super().__init__()\n",
        "        self.fc1=nn.Linear(in_,h1)\n",
        "        self.fc2=nn.Linear(h1,h2)\n",
        "        self.fc3=nn.Linear(h2,out_)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=F.relu(self.fc2(x))\n",
        "        x=self.fc3(x)\n",
        "        return x\n",
        "    \n",
        "class Model2(nn.Module):\n",
        "    def __init__(self, in_,h1,h2,h3, out_,p=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout=nn.Dropout(p=p)\n",
        "        self.fc1=nn.Linear(in_,h1)\n",
        "        self.fc2=nn.Linear(h1,h2)\n",
        "        self.fc3=nn.Linear(h2,h3)\n",
        "        self.fc4=nn.Linear(h3,out_)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.dropout(F.relu(self.fc1(x)))\n",
        "        x=self.dropout(F.relu(self.fc2(x)))\n",
        "        x=self.dropout(F.relu(self.fc3(x)))\n",
        "        x=self.fc4(x)\n",
        "        return x\n",
        "\n",
        "iris_model=Model(frequency_matrix_count_1.shape[1],128,64,25)\n",
        "iris_model=Model2(frequency_matrix_count_1.shape[1],128,64,32,25,p=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k84U60joolJI"
      },
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(iris_model.parameters(),lr=.01)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YYqabCKolJI"
      },
      "source": [
        "criterion.to(device)\n",
        "iris_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYwQL5gLolJJ"
      },
      "source": [
        "optimizer.zero_grad()\n",
        "loss_=[]\n",
        "epochs=25\n",
        "for epoch in range(epochs):\n",
        "    for x,y in iris_loader:\n",
        "        print(\"hi\")\n",
        "\n",
        "        y_pred=iris_model(x.to(device))\n",
        "        y_pred.shape\n",
        "        loss=criterion(y_pred.to(device),y.to(device))\n",
        "\n",
        "        loss_.append(loss)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            t,i=torch.max(nn.functional.log_softmax(y_pred.cpu(),dim = 1),1)\n",
        "            acc=f1_score(y,i,average='weighted')\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"epoch {epoch}, loss {loss.item()} , accuracy:{acc}\")\n",
        "\n",
        "classification_rep(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A_85f6BolJJ"
      },
      "source": [
        "## real models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKf28NRbolJJ"
      },
      "source": [
        "SEED = 1234\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "df = pd.read_excel(\"train.xlsx\")\n",
        "df = df[['Description', 'label']]\n",
        "df[\"desc\"] = df[\"Description\"]\n",
        "df.drop([\"Description\"], axis=1, inplace=True)\n",
        "df.columns = [\"label\", \"text\"]\n",
        "df.to_csv('thedata.csv', index=False)\n",
        "\n",
        "\n",
        "data=pd.read_excel(\"train.xlsx\")\n",
        "np.random.seed(10)\n",
        "remove_n=600\n",
        "drop_indices = np.random.choice(data[data[\"label\"]==2].index, remove_n, replace=False)\n",
        "data = data.drop(drop_indices)\n",
        "\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True, pad_first=True, tokenize=tokenize_digits)\n",
        "LABEL = data.Field(sequential=False, use_vocab=False, is_target=True)\n",
        "\n",
        "dataset = data.TabularDataset(path='thedata.csv', format='csv', skip_header=True,\n",
        "                              fields=[('label', LABEL), ('text', TEXT)]\n",
        "                              )\n",
        "\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=.75)  # default is 0.7\n",
        "train_dataset, valid_dataset = train_dataset.split(random_state=random.seed(SEED), split_ratio=.90)\n",
        "len(train_dataset.examples)\n",
        "len(valid_dataset.examples)\n",
        "len(test_dataset.examples)\n",
        "\n",
        "ex = dataset.examples[3]\n",
        "ex.text\n",
        "ex.label\n",
        "\n",
        "train_dataset.examples[0].text\n",
        "vars(train_dataset[0])\n",
        "\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "\n",
        "TEXT.build_vocab(train_dataset,\n",
        "                 max_size=MAX_VOCAB_SIZE,\n",
        "                 # vectors=\"glove.6B.300d\",\n",
        "                 # unk_init=torch.Tensor.normal_,\n",
        "                 min_freq=10,\n",
        "                 )\n",
        "\n",
        "LABEL.build_vocab(train_dataset)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset, test_dataset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=False,\n",
        "    device=device)\n",
        "\n",
        "for inputs, targets in train_iterator:\n",
        "    print(\"imput:\", inputs.shape, \"shape:\", targets.shape)\n",
        "    break\n",
        "\n",
        "for batch in valid_iterator:\n",
        "    print(batch.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHGnF0F2olJK"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n",
        "                 dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1,\n",
        "                      out_channels=n_filters,\n",
        "                      kernel_size=(2, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text = [sent len, batch size]\n",
        "\n",
        "        # text = text.permute(1, 0)\n",
        "\n",
        "        # text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # embedded = [batch size, sent len, emb dim]\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "\n",
        "        # embedded = [batch size, 1, sent len, emb dim]\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        # conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        # pooled_n = [batch size, n_filters]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs):\n",
        "        super(RNN, self).__init__()\n",
        "        self.V = n_vocab\n",
        "        self.D = embed_dim\n",
        "        self.M = n_hidden\n",
        "        self.K = n_outputs\n",
        "        self.L = n_rnnlayers\n",
        "\n",
        "        self.embed = nn.Embedding(self.V, self.D)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.D,\n",
        "            hidden_size=self.M,\n",
        "            num_layers=self.L,\n",
        "            batch_first=True)\n",
        "        self.fc = nn.Linear(self.M, self.K)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # initial hidden states\n",
        "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "        c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "\n",
        "        # embedding layer\n",
        "        # turns word indexes into word vectors\n",
        "        out = self.embed(X)\n",
        "\n",
        "        # get RNN unit output\n",
        "        out, _ = self.rnn(out, (h0, c0))\n",
        "\n",
        "        # max pool\n",
        "        out, _ = torch.max(out, 1)\n",
        "\n",
        "        # we only want h(T) at the final time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CNN2(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_dim, n_outputs):\n",
        "        super(CNN2, self).__init__()\n",
        "        self.V = n_vocab\n",
        "        self.D = embed_dim\n",
        "        self.K = n_outputs\n",
        "\n",
        "        # if input is T words\n",
        "        # then output is (T, D) matrix\n",
        "        self.embed = nn.Embedding(self.V, self.D)\n",
        "\n",
        "        # conv layers\n",
        "        self.conv1 = nn.Conv1d(self.D, 32, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        self.conv2 = nn.Conv1d(32, 64, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "        self.conv3 = nn.Conv1d(64, 128, 3, padding=1)\n",
        "\n",
        "        self.fc = nn.Linear(128, self.K)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # embedding layer\n",
        "        # turns word indexes into word vectors\n",
        "        out = self.embed(X)\n",
        "\n",
        "        # note: output of embedding is always\n",
        "        # (N, T, D)\n",
        "        # conv1d expects\n",
        "        # (N, D, T)\n",
        "\n",
        "        # conv layers\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = self.conv1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.pool1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.pool2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        # change it back\n",
        "        out = out.permute(0, 2, 1)\n",
        "\n",
        "        # max pool\n",
        "        out, _ = torch.max(out, 1)\n",
        "\n",
        "        # final dense layer\n",
        "        out = self.fc(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JBnngfholJL"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "N_FILTERS = 50\n",
        "FILTER_SIZES = [2, 3, 4, 5, 6, 7]\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "DROPOUT = 0.2\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "# model = RNN(len(TEXT.vocab), EMBEDDING_DIM, 15, 1, OUTPUT_DIM)\n",
        "# model = CNN2(len(TEXT.vocab), EMBEDDING_DIM, OUTPUT_DIM)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iN8z39jolJL"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "optimizer = optim.Adam(model.parameters(), lr=.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "N_EPOCHS = 50\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = np.zeros(N_EPOCHS)\n",
        "test_losses = np.zeros(N_EPOCHS)\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvzX34tyolJM"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    optimizer.zero_grad()\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "\n",
        "    # Save losses\n",
        "    train_losses[epoch] = train_loss\n",
        "    test_losses[epoch] = valid_loss\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKjEKVK5olJM"
      },
      "source": [
        "## Another simmilar example with schedueler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGXz5VOdolJM",
        "outputId": "57011483-f05d-46e0-a834-dc441d433881"
      },
      "source": [
        "MAX_VOCAB_SIZE = 5000\n",
        "BATCH_SIZE = 512\n",
        "MIN_FREQ=5\n",
        "splt=0.8\n",
        "tkz=tokenize_plus2\n",
        "# <editor-fold desc=\"boiler-plate\">\n",
        "SEED = 1234\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "df=pd.read_excel(\"train.xlsx\")\n",
        "np.random.seed(10)\n",
        "remove_n=600\n",
        "drop_indices = np.random.choice(df[df[\"label\"]==2].index, remove_n, replace=False)\n",
        "df = df.drop(drop_indices)\n",
        "df = df[['Description', 'label']]\n",
        "df[\"desc\"] = df[\"Description\"]\n",
        "df.drop([\"Description\"], axis=1, inplace=True)\n",
        "df.columns = [\"label\", \"text\"]\n",
        "df.to_csv('thedata-notrip.csv', index=False)\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True, pad_first=True, tokenize=tkz)\n",
        "LABEL = data.Field(sequential=False, use_vocab=False, is_target=True)\n",
        "dataset = data.TabularDataset(path='thedata-notrip.csv', format='csv', skip_header=True,\n",
        "                              fields=[('label', LABEL), ('text', TEXT)]\n",
        "                              )\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=splt)  # default is 0.7\n",
        "train_dataset, valid_dataset = train_dataset.split(random_state=random.seed(SEED), split_ratio=splt)\n",
        "len(dataset)\n",
        "\n",
        "\n",
        "TEXT.build_vocab(train_dataset,\n",
        "                 max_size=MAX_VOCAB_SIZE,\n",
        "                 # vectors=\"glove.6B.300d\",\n",
        "                 # unk_init=torch.Tensor.normal_,\n",
        "                 min_freq=MIN_FREQ\n",
        "                 )\n",
        "\n",
        "LABEL.build_vocab(train_dataset)\n",
        "\n",
        "\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset, test_dataset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    # sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=False,\n",
        "    sort=False,\n",
        "    device=device)\n",
        "\n",
        "\n",
        "e=nn.Embedding(5, 4)\n",
        "t=np.random.randint(1,5,[3,2])\n",
        "tt=torch.LongTensor(t)\n",
        "e(tt)#3x2 ---> 3x2x4\n",
        "# </editor-fold>\n",
        "DROPOUT = 0.5\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenize_plus2' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-0e13bc52c620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mMIN_FREQ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msplt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtkz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenize_plus2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# <editor-fold desc=\"boiler-plate\">\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mSEED\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1234\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tokenize_plus2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpJv0pIPolJN"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim,\n",
        "                 dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1,\n",
        "                      out_channels=n_filters,\n",
        "                      kernel_size=(2, embedding_dim))\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text = [sent len, batch size]\n",
        "\n",
        "        # text = text.permute(1, 0)\n",
        "\n",
        "        # text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # embedded = [batch size, sent len, emb dim]\n",
        "\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "\n",
        "        # embedded = [batch size, 1, sent len, emb dim]\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "\n",
        "        # conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        # pooled_n = [batch size, n_filters]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs,p=.5):\n",
        "        super(RNN, self).__init__()\n",
        "        self.V = n_vocab\n",
        "        self.D = embed_dim\n",
        "        self.M = n_hidden\n",
        "        self.K = n_outputs\n",
        "        self.L = n_rnnlayers\n",
        "\n",
        "        self.dropout=nn.Dropout(p=p)\n",
        "\n",
        "        self.embed = nn.Embedding(self.V, self.D)\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.D,\n",
        "            hidden_size=self.M,\n",
        "            num_layers=self.L,\n",
        "            batch_first=True)\n",
        "        self.fc = nn.Linear(self.M, self.K)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # initial hidden states\n",
        "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "        c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
        "\n",
        "        # embedding layer\n",
        "        # turns word indexes into word vectors\n",
        "        out = self.embed(X)\n",
        "\n",
        "        # get RNN unit output\n",
        "        out, _ = self.rnn(out, (h0, c0))\n",
        "\n",
        "        out=self.dropout(out)\n",
        "\n",
        "        # max pool\n",
        "        out, _ = torch.max(out, 1)\n",
        "\n",
        "        # we only want h(T) at the final time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class CNN2(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_dim, n_outputs):\n",
        "        super(CNN2, self).__init__()\n",
        "        self.V = n_vocab\n",
        "        self.D = embed_dim\n",
        "        self.K = n_outputs\n",
        "\n",
        "        # if input is T words\n",
        "        # then output is (T, D) matrix\n",
        "        self.embed = nn.Embedding(self.V, self.D)\n",
        "\n",
        "        # conv layers\n",
        "        self.conv1 = nn.Conv1d(self.D, 32, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        self.conv2 = nn.Conv1d(32, 64, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "        self.conv3 = nn.Conv1d(64, 128, 3, padding=1)\n",
        "\n",
        "        self.fc = nn.Linear(128, self.K)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # embedding layer\n",
        "        # turns word indexes into word vectors\n",
        "        out = self.embed(X)\n",
        "\n",
        "        # note: output of embedding is always\n",
        "        # (N, T, D)\n",
        "        # conv1d expects\n",
        "        # (N, D, T)\n",
        "\n",
        "        # conv layers\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = self.conv1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.pool1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.pool2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        # change it back\n",
        "        out = out.permute(0, 2, 1)\n",
        "\n",
        "        # max pool\n",
        "        out, _ = torch.max(out, 1)\n",
        "\n",
        "        # final dense layer\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class CNN3(nn.Module):\n",
        "    def __init__(self, n_vocab, embed_dim, n_outputs,p=.1):\n",
        "        super(CNN3, self).__init__()\n",
        "        self.V = n_vocab\n",
        "        self.D = embed_dim\n",
        "        self.K = n_outputs\n",
        "\n",
        "        # if input is T words\n",
        "        # then output is (T, D) matrix\n",
        "        self.embed = nn.Embedding(self.V, self.D)\n",
        "        self.dropout=nn.Dropout(p)\n",
        "\n",
        "        # conv layers\n",
        "        self.conv1 = nn.Conv1d(self.D, 64, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        self.conv2 = nn.Conv1d(64, 128, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "        self.conv3 = nn.Conv1d(128, 64, 3, padding=1)\n",
        "\n",
        "        self.fc = nn.Linear(64, self.K)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # embedding layer\n",
        "        # turns word indexes into word vectors\n",
        "        out = self.embed(X)\n",
        "\n",
        "        # note: output of embedding is always\n",
        "        # (N, T, D)\n",
        "        # conv1d expects\n",
        "        # (N, D, T)\n",
        "\n",
        "        # conv layers\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = self.conv1(out)\n",
        "        out = self.dropout(F.relu(out))\n",
        "        out = self.pool1(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.dropout(F.relu(out))\n",
        "        out = self.pool2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        # change it back\n",
        "        out = out.permute(0, 2, 1)\n",
        "\n",
        "        # max pool\n",
        "        out, _ = torch.max(out, 1)\n",
        "\n",
        "        # final dense layer\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydH2DAkLolJO"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "N_FILTERS = 50\n",
        "FILTER_SIZES = [2, 3, 4, 5, 6, 7]\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "LR=.0005\n",
        "\n",
        "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "# model = CNN2(len(TEXT.vocab), EMBEDDING_DIM, OUTPUT_DIM)\n",
        "# model = RNN(len(TEXT.vocab), EMBEDDING_DIM, 15, 1, OUTPUT_DIM,p=DROPOUT)\n",
        "# model = CNN3(len(TEXT.vocab), EMBEDDING_DIM, OUTPUT_DIM,p=DROPOUT)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR,weight_decay=1e-4)\n",
        "scheduler=ReduceLROnPlateau(optimizer,\"min\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "N_EPOCHS = 100\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_losses = np.zeros(N_EPOCHS)\n",
        "test_losses = np.zeros(N_EPOCHS)\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5qjsfXColJO"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    optimizer.zero_grad()\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train_(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    scheduler.step(valid_loss)\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "\n",
        "    # Save losses\n",
        "    train_losses[epoch] = train_loss\n",
        "    test_losses[epoch] = valid_loss\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "\n",
        "predictions=classification_rep(X_test,y_test)\n",
        "plot_confusion_matrix_(data,predictions)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}